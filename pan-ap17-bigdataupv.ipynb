{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_docs(author, id, genero, variedad):\n",
    "    author_doc = {}\n",
    "    author_doc['id'] = id\n",
    "    author_doc['genero'] = genero\n",
    "    author_doc['variedad'] = variedad\n",
    "    \n",
    "    total = 0\n",
    "    tweet = ''\n",
    "    for doc in author.iter('document'):\n",
    "        doc_dict = author_doc.copy()\n",
    "        tweet = tweet +  \" \" + doc.text.replace(\"\\n\", \"\")\n",
    "        total += 1\n",
    "    \n",
    "    author_doc['total'] = total\n",
    "    author_doc['tweet'] = tweet\n",
    "    return author_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 1, column 1 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/usuario/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-12-75ed31be8b81>\"\u001b[0m, line \u001b[1;32m15\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    parsed = et.parse(f)\n",
      "  File \u001b[1;32m\"/home/usuario/anaconda3/lib/python3.6/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1196\u001b[0m, in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/usuario/anaconda3/lib/python3.6/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m597\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 1, column 1\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "truth = {}\n",
    "\n",
    "training = 'training'\n",
    "test = 'test'\n",
    "\n",
    "for dirname in [training, test]:\n",
    "    data[dirname] = pd.DataFrame()\n",
    "    truth[dirname] = pd.read_csv('./'+dirname+'/truth.txt', sep=\":::\", names = [\"id\", \"genero\", \"pais\"], header=None, engine='python')\n",
    "\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename == \"truth.txt\":\n",
    "            continue\n",
    "        f = './'+dirname+'/'+filename\n",
    "        parsed = et.parse(f)\n",
    "        \n",
    "        t = truth[dirname].loc[truth[dirname]['id'] == filename.replace(\".xml\", \"\")]\n",
    "        for index, row in t.iterrows():\n",
    "            genero = row[1]\n",
    "            variedad = row[2]\n",
    "\n",
    "        data[dirname] = data[dirname].append([iter_docs(parsed.getroot(), filename.replace(\".xml\", \"\"), genero, variedad)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "\n",
    "def clean(text):\n",
    "    tweet = text.replace(\"\\\\n\", \"\")\n",
    "    #tokenizamos el tweet\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #quitmos acentos\n",
    "    return unidecode.unidecode(\" \".join(tokens))\n",
    "\n",
    "for dirname in [training, test]:\n",
    "    data[dirname]['ctweet'] = data[dirname].apply(lambda row: clean(row['tweet']), axis=1)\n",
    "\n",
    "data[training].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nuevas caracter√≠sticas\n",
    "emoticons_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
    "def emoticons(text):\n",
    "    count = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for t in tokens:\n",
    "        search = emoticons_pattern.findall(t)\n",
    "        count += len(search)\n",
    "    return count\n",
    "\n",
    "mentions_pattern = re.compile(\"@\\w+\")\n",
    "def mentions(text):\n",
    "    users = mentions_pattern.findall(text)\n",
    "    return len(users)\n",
    "\n",
    "hashtags_pattern = re.compile(\"#\\w+\")\n",
    "def hashtags(text):\n",
    "    hashes = hashtags_pattern.findall(text)\n",
    "    return len(hashes)\n",
    "\n",
    "url_pattern = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "def readurl(text):\n",
    "    urls = url_pattern.findall(text)\n",
    "    return len(urls)\n",
    "\n",
    "def tokenscount(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "retweets_pattern = re.compile(\"(RT|via)((?:\\b\\W*@\\w+)+)\")\n",
    "def retweets(text):\n",
    "    rts = retweets_pattern.findall(text)\n",
    "    return len(rts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21605dbd07be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransformaciones\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-21605dbd07be>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[A-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1cafd144b50d>\u001b[0m in \u001b[0;36memoticons\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memoticons_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "transformaciones = [\n",
    "    lambda x: len(x),\n",
    "    lambda x: x.count(\" \"),\n",
    "    lambda x: x.count(\".\"),\n",
    "    lambda x: x.count(\"!\"),\n",
    "    lambda x: x.count(\"?\"),\n",
    "    lambda x: len(x) / (x.count(\" \") + 1),\n",
    "    lambda x: x.count(\" \") / (x.count(\".\") + 1),\n",
    "    lambda x: len(re.findall(\"\\d\", x)),\n",
    "    lambda x: len(re.findall(\"[A-Z]\", x)),\n",
    "    lambda x: emoticons(x),\n",
    "    lambda x: mentions(x),\n",
    "    lambda x: hashtags(x),\n",
    "    lambda x: readurl(x),\n",
    "    lambda x: tokenscount(x),\n",
    "    lambda x: retweets(x)\n",
    "]\n",
    "\n",
    "meta = {}\n",
    "for d in [training, test]:\n",
    "    meta[d] = []\n",
    "    for func in transformaciones:\n",
    "        meta[d].append(data[d]['tweet'].apply(func)/data[d]['total'])\n",
    "        \n",
    "meta[training] = np.asarray(meta[training]).T\n",
    "meta[test] = np.asarray(meta[test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.18540000e+02,   1.61400000e+01,   6.40000000e-01,\n",
       "          5.20000000e-01,   7.00000000e-02,   7.33993808e-02,\n",
       "          2.48307692e-01,   6.90000000e-01,   7.44000000e+00,\n",
       "          5.00000000e-02,   1.12000000e+00,   8.80000000e-01,\n",
       "          3.40000000e-01,   1.65400000e+01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[training][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import time\n",
    "from sklearn import decomposition\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printModels():\n",
    "    for key in models:\n",
    "        start = time.time()\n",
    "        model = models[key].fit(features, y_train.ravel())\n",
    "        predicted = model.predict(features_test)\n",
    "        end = time.time()\n",
    "        print(key, accuracy_score(y_test,predicted), \"Time:\", end-start)\n",
    "        \n",
    "        \n",
    "stemmer = PorterStemmer()\n",
    "trans_table = {ord(c): None for c in string.punctuation + string.digits} \n",
    "def tokenize(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text.translate(trans_table)) if len(word) > 1] #if len(word) > 1 because I only want to retain words that are at least two characters before stemming, although I can't think of any such words that are not also stopwords\n",
    "    stems = [stemmer.stem(item) for item in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 118.54   16.14    0.64 ...,    0.      0.      0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#genero\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=stopwords.words('spanish'),\\\n",
    "                            ngram_range=(1,2), min_df=10, max_features=8000,norm='l2',\\\n",
    "                            use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "  \n",
    "y_train=data[training]['genero'].values\n",
    "X_train = vectorizer.fit_transform(data[training]['ctweet'])\n",
    "\n",
    "features = np.hstack([meta[training], X_train.todense()])\n",
    "                      \n",
    "#creando bolsa de palabras por g√©nero\n",
    "#data['male'] = data[training][data[training].genero=='male']\n",
    "#data['female'] = data[training][data[training].genero=='female']\n",
    "\n",
    "#vectorizer_male = TfidfVectorizer(analyzer=\"word\", max_features=200, tokenizer=tokenize)\n",
    "#vectorizer_male.fit(data['male']['ctweet'])\n",
    "#X_train_male = vectorizer_male.transform(data[training]['ctweet'])\n",
    "\n",
    "#vectorizer_female = TfidfVectorizer(analyzer=\"word\", max_features=200, tokenizer=tokenize)\n",
    "#vectorizer_female.fit(data['female']['ctweet'])\n",
    "#X_train_female = vectorizer_female.transform(data[training]['ctweet'])\n",
    "\n",
    "#features = np.hstack([meta[training], X_train.todense(), X_train_male.todense(), X_train_female.todense()])\n",
    "\n",
    "print(features[:1])\n",
    "\n",
    "y_test=data[test]['genero'].values\n",
    "X_test=vectorizer.transform(data[test]['ctweet'])\n",
    "\n",
    "#creando bolsa de palabras por g√©nero\n",
    "#X_test_male = vectorizer_male.transform(data[test]['ctweet'])\n",
    "#X_test_female = vectorizer_female.transform(data[test]['ctweet'])\n",
    "\n",
    "#features_test = np.hstack([meta[test], X_test.todense(), X_test_male.todense(), X_test_female.todense()])\n",
    "features_test = np.hstack([meta[test], X_test.todense()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 0.592142857143 Time: 3.0182721614837646\n",
      "NB 0.686428571429 Time: 0.1158747673034668\n",
      "R 0.645714285714 Time: 0.7247958183288574\n",
      "KN 0.577142857143 Time: 6.471596002578735\n",
      "LO 0.769285714286 Time: 0.6107280254364014\n",
      "NE 0.775714285714 Time: 75.25347208976746\n"
     ]
    }
   ],
   "source": [
    "models={\n",
    "    \"L\": LinearSVC(),\n",
    "    \"NB\": MultinomialNB(),\n",
    "    \"R\": RandomForestClassifier(),\n",
    "    \"KN\": KNeighborsClassifier(),\n",
    "    \"LO\": LogisticRegression(),\n",
    "    \"NE\": MLPClassifier()\n",
    "}\n",
    "\n",
    "    \n",
    "printModels()\n",
    "\n",
    "#con bolsa de palabras por g√©nero\n",
    "#L 0.681428571429 Time: 4.463599920272827\n",
    "#NB 0.677857142857 Time: 0.08524703979492188\n",
    "#R 0.673571428571 Time: 0.7116379737854004\n",
    "#KN 0.587142857143 Time: 6.8814311027526855\n",
    "#LO 0.775 Time: 0.902728796005249\n",
    "#NE 0.774285714286 Time: 69.29499626159668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variedad del castellano\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=stopwords.words('spanish'),\\\n",
    "                            min_df=50, max_features=8000,norm='l2',\\\n",
    "                            use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "  \n",
    "y_train=data[training]['variedad'].values\n",
    "X_train = vectorizer.fit_transform(data[training]['ctweet'])\n",
    "\n",
    "y_test=data[test]['variedad'].values\n",
    "X_test=vectorizer.transform(data[test]['ctweet'])\n",
    "\n",
    "features = np.hstack([meta[training], X_train.todense()])\n",
    "features_test = np.hstack([meta[test], X_test.todense()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 0.676428571429 Time: 14.360452890396118\n",
      "NB 0.732142857143 Time: 0.04561209678649902\n",
      "R 0.831428571429 Time: 0.46947312355041504\n",
      "KN 0.23 Time: 3.02897310256958\n",
      "LO 0.936428571429 Time: 2.371469020843506\n",
      "NE 0.937857142857 Time: 23.478174924850464\n"
     ]
    }
   ],
   "source": [
    "models={\n",
    "    \"L\": LinearSVC(),\n",
    "    \"NB\": MultinomialNB(),\n",
    "    \"R\": RandomForestClassifier(),\n",
    "    \"KN\": KNeighborsClassifier(),\n",
    "    \"LO\": LogisticRegression(),\n",
    "    \"NE\": MLPClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "printModels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
